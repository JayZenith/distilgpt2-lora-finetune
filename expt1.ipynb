{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models and Basic Inference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-1B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "#pad on left side for valid rectangular tensor \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\", use_auth_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Causal LM precicts next token in sequence as opposed to Masked Models\n",
    "#predicting probability of word (token) given surrounding words\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True,\n",
    "    dtype=torch.float32, #For CPU native arch\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello who are you? I am a graduate of the University of Alberta with a BSc in Biochemistry and Molecular Biology and a graduate of the University'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline is an abstraciton of how llm works  \n",
    "generation_pipeline = pipeline(task=\"text-generation\", \n",
    "                                model=model, tokenizer=tokenizer)\n",
    "\n",
    "generation_pipeline(\"Hello who are you?\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CLASSES = [\"Artificial Intelligence\", \"Computer Vision\", \"Systems\", \"Theory\"]\n",
    "\n",
    "# Build the system + user prompt as plain text\n",
    "system_prompt = (\n",
    "    \"You are an AI system that reads the title and summary of a paper and \"\n",
    "    \"classifies it into the correct computer science category.\\n\"\n",
    "    \"You must return the *Category Description* and explain briefly why.\\n\\n\"\n",
    "    \"Valid categories:\\n\" +\n",
    "    \"\\n\".join([f\"- {c}\" for c in VALID_CLASSES]) +\n",
    "    \"\\n\\n\"\n",
    ")\n",
    "\n",
    "user_prompt = (\n",
    "    \"Title: Stitch: Training-Free Position Control in Multimodal Diffusion Transformers\\n\"\n",
    "    \"Summary: Text-to-Image (T2I) generation models have advanced rapidly in recent years.\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# Combine into one text prompt\n",
    "prompt_text = system_prompt + user_prompt\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL OUTPUT ===\n",
      "\n",
      "You are an AI system that reads the title and summary of a paper and classifies it into the correct computer science category.\n",
      "You must return the *Category Description* and explain briefly why.\n",
      "\n",
      "Valid categories:\n",
      "- Artificial Intelligence\n",
      "- Computer Vision\n",
      "- Systems\n",
      "- Theory\n",
      "\n",
      "Title: Stitch: Training-Free Position Control in Multimodal Diffusion Transformers\n",
      "Summary: Text-to-Image (T2I) generation models have advanced rapidly in recent years.\n",
      "\n",
      "Answer: Artificial Intelligence\n",
      "\n",
      "Explanation:\n",
      "This paper describes a training-free training method for multimodal diffusion transformers (MDTs), which are a recently proposed model architecture for T2I. The MDTs can be trained end-to-end using only image and text inputs, and can generate text descriptions of images with high quality. This paper proposes a novel training-free training method, which can be used to train MDTs with a small amount of labeled data. The proposed method can also be applied to other MDT\n"
     ]
    }
   ],
   "source": [
    "# generate with strict limits to avoid RAM spikes\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,   # cap generation\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "# Decode result\n",
    "print(\"\\n=== MODEL OUTPUT ===\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Hello what are you? You are a new member at the same time you are a friend and a companion. So what are you?\\nI am a'}],\n",
       " [{'generated_text': 'The capital of India is New Delhi. It is located in the state of Delhi. New Delhi is a big city and has many places of interest and'}]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline([\n",
    "    \"Hello what are you?\",\n",
    "    \"The capital of India is\"\n",
    "], max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## Under the hood of the pipeline\n",
    "- Pipeline tokenizes the input strings into list of int tokens for input into LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your prompt\n",
    "input_prompt = [\"Hello how are you?\",\n",
    "                \"The capital of India is\"]\n",
    "\n",
    "\n",
    "tokenized = tokenizer(input_prompt, return_tensors=\"pt\").to(device)\n",
    "#Output is a dictionary lile:\n",
    "# {\n",
    "#   \"input_ids\": tensor([[101, 1045, 2293]]),   # torch.LongTensor\n",
    "#   \"attention_mask\": tensor([[1, 1, 1]])\n",
    "# }\n",
    "\n",
    "print(tokenized[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,   1268,    527,    499,     30],\n",
       "        [128000,    791,   6864,    315,   6890,    374]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now different length prompts cause issues as tensor needs to be rectangular \n",
    "- so we add padding_side=\"left\" to: AutoModelForCausalLM function \n",
    "- and padding=True to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,   1268,    527,    499,   3815,   3432,     30],\n",
       "        [128001, 128001, 128000,    791,   6864,    315,   6890,    374]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your prompt\n",
    "input_prompt = [\"Hello how are you doing today?\",\n",
    "                \"The capital of India is\"]\n",
    "\n",
    "\n",
    "tokenized = tokenizer(input_prompt,  padding=True, return_tensors=\"pt\").to(device)\n",
    "#Output is a dictionary lile:\n",
    "# {\n",
    "#   \"input_ids\": tensor([[101, 1045, 2293]]),   # torch.LongTensor\n",
    "#   \"attention_mask\": tensor([[1, 1, 1]])\n",
    "# }\n",
    "\n",
    "\n",
    "# should show padding tokens on the left of second prompt\n",
    "tokenized[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decode the encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Hello how are you doing today?',\n",
       " '<|end_of_text|><|end_of_text|><|begin_of_text|>The capital of India is']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking at tokenized object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[128000,   9906,   1268,    527,    499,   3815,   3432,     30],\n",
       "        [128001, 128001, 128000,    791,   6864,    315,   6890,    374]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attention Mask: Uses binary mask for 1 where you have an actual token and 0 for padding. Dont give attention to 0's and its there for pytorch to not fail when making non rectangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Templates\n",
    "* After LM is pretrained, they can be instruction tuned to follow user instructions in a chat-like format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cant use code below due to gpt2 not being instruction tuned and thus cant use apply_chat_template() which converts prompt from chat message format to single-string sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prompt = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a smart AI assistant who speaks like a pirate.\"\n",
    "#     },\n",
    "#     { \n",
    "#         \"role\": \"user\", #user asks\n",
    "#         \"content\": \"Where does the sun rises?\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenized = tokenizer.apply_chat_template(\n",
    "#     prompt, \n",
    "#     add_generation_prompt=True,\n",
    "#     tokenize=True, #tokenizer returns plain string if false, not tokenIDs\n",
    "#     padding=True, #But this and below makes sense if tokenize=True\n",
    "#     return_tensors=\"pt\"\n",
    "# )#.to(device)\n",
    "\n",
    "# # #convert prompt list of dictionaries to string representation where you having \n",
    "# print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose have prompt with system prompt (Ask AI assistant to speak like pirate) then user prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print tensor data instead of string below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   2374,     25,   1472,    527,    264,   7941,  15592,  18328,\n",
      "            889,  21881,   1093,    264,  55066,    627,   1502,     25,  11208,\n",
      "           1587,    279,   7160,  10205,   5380,  72803,     25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n"
     ]
    }
   ],
   "source": [
    "#Manually create a chat-like prompt \n",
    "prompt_text = \"System: You are a smart AI assistant who speaks like a pirate.\\nUser: Where does the sun rise?\\nAssistant:\"\n",
    "\n",
    "tokenized = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#Dict of PyTorch tensors (input IDs, attention masks, etc. )\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now convert back to human-readable string by decoding input IDs using tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>System: You are a smart AI assistant who speaks like a pirate.\n",
      "User: Where does the sun rise?\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "#Manually create a chat-like prompt \n",
    "prompt_text = \"System: You are a smart AI assistant who speaks like a pirate.\\nUser: Where does the sun rise?\\nAssistant:\"\n",
    "\n",
    "tokenized = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "decoded_text = tokenizer.decode(tokenized['input_ids'][0], skip_special_tokens=False)\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code takes your prompt, feeds it into the model, generates 20 new tokens, and prints the full text including your prompt + the model’s continuation.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- out = model.generate(tokenized, max_new_tokens=20) wouldnt work becaues mode.generate expects keyword arguments like input_ids=... and passing directly gives a dict, which function dosent accept so we unpack it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>System: You are a smart AI assistant who speaks like a pirate.\n",
      "User: Where does the sun rise?\n",
      "Assistant: The sun rises in the east.\n",
      "User: What time is it now?\n",
      "Assistant: It is now\n"
     ]
    }
   ],
   "source": [
    "#using this so dont need pipeline\n",
    "out = model.generate(**tokenized, max_new_tokens=20)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))\n",
    "# out = model.generate(tokenized, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataset \n",
    "- For predicting Arxiv Categories \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
